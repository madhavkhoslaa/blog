<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Caching: My lessons on it - Madhav Khosla</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Caching is a cheap way to scale a system's performance in very less time. Being cheap and easy it also brings some problems such as invalidation and infrastructure for setting up the cache itself. Here I plan to talk about some lessons on caching that I've learnt in a few months."><meta property="og:image" content><meta property="og:title" content="Caching: My lessons on it"><meta property="og:description" content="Caching is a cheap way to scale a system's performance in very less time. Being cheap and easy it also brings some problems such as invalidation and infrastructure for setting up the cache itself. Here I plan to talk about some lessons on caching that I've learnt in a few months."><meta property="og:type" content="article"><meta property="og:url" content="https://madhavkhoslaa.github.io/posts/post1/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-18T02:01:58+05:30"><meta property="article:modified_time" content="2022-03-18T02:01:58+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Caching: My lessons on it"><meta name=twitter:description content="Caching is a cheap way to scale a system's performance in very less time. Being cheap and easy it also brings some problems such as invalidation and infrastructure for setting up the cache itself. Here I plan to talk about some lessons on caching that I've learnt in a few months."><script src=https://madhavkhoslaa.github.io/js/feather.min.js></script>
<link href=https://madhavkhoslaa.github.io/css/fonts.b685ac6f654695232de7b82a9143a46f9e049c8e3af3a21d9737b01f4be211d1.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://madhavkhoslaa.github.io/css/main.40ca3a860425083862b7ebd55447caec5c4384573f0cb098b8d06a91e8dace2e.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://madhavkhoslaa.github.io/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css></head><body><div class=content><header><div class=main><a href=https://madhavkhoslaa.github.io/>Madhav Khosla</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=title><h1 class=title>Caching: My lessons on it</h1><div class=meta>Posted on Mar 18, 2022</div></div><section class=body><p>Caching is a cheap way to scale a system&rsquo;s performance in very less time. Being cheap and easy to implement over a system it also brings some problems such as invalidation and infrastructure for setting up the cache itself. Here I plan to talk about some lessons on caching that I&rsquo;ve learnt in a few months.</p><h1 id=philosophy-of-caching>Philosophy of Caching</h1><p><em>The supreme art of war is to subdue the enemy without fighting it - Sun Tzu</em></p><h1 id=caching>Caching</h1><p>Caching in a pure function system is when you store the arguments to a particular function and use the result stored instead of actually invoking the function and for a good approach to caching you might want to design your caching scheme in such a way that for most responses the cache is actually used instead of performing a expensive operation again.
This need of reusability of cache is called increasing the &ldquo;Cache-Hit Ratio&rdquo;. We will talk about &ldquo;Cache-Hit Ratio&rdquo; now and ways to maximise it.</p><h1 id=cache-hit-ratio>Cache-Hit Ratio</h1><p>Mathematically it is the ratio of cache used to total objects cached in your system; But what does it convey to you ?
A high cache hit ratio means that your cache was used for most objects that were cached and hence you saved alot of CPU time for logic that was already computed. So how do you achieve a high Cache-Hit Ratio ?</p><h1 id=methodlogies-for-increasing-the-cache-hit-ratio>Methodlogies for increasing the &ldquo;Cache-Hit Ratio&rdquo;</h1><ol><li><strong>Cache Key Space</strong> - Describes the number of keys in your cache if you have a key value based cache sysem.</li><li><strong>Average TTL</strong> - How long a particular object is cached for, you just don&rsquo;t want to store objects forever since storage costs $$, Longer the TTL the better, You do not have to worry alot since algoritms like LRU and so on manage this for you.</li><li><strong>Average size of cached object</strong> - This affects the first object in this list due to hardware constricts you can obviously store a certain amount of objects depending upon their size. The larger the size of the object the lesser the Cache Key Space it, Hence it might lead to a lower Cache-Hit Ratio</li></ol><h2 id=how-to-achieve-a-large-cache-key-space->How to achieve a large Cache Key Space ?</h2><ol><li>Choosing a scheme in which cache is common among users</li><li>Keeping the size of the cached object such that more objects could be stored and hardware is not a construct.</li></ol><h1 id=cache-rule-of-thumb>Cache rule of thumb</h1><ol><li>Cache high up the call stack</li><li>Reuse cache among users</li></ol><h1 id=where-to-cache-cache-high-up-the-call-stack>Where to cache (Cache high up the call stack)?</h1><p><img src=https://i.imgur.com/xIFu5IS.jpeg alt=image>
In this scenario imagine you have 3 functions calling each other in the call stack and the result is produced by calling them all.
Assume you call f(a) and inside the implementation it passes the value to f(f(a)) and so on.
I am assuming the cost of each operation likewise in the table.</p><table><thead><tr><th>Function</th><th>Time</th></tr></thead><tbody><tr><td>f(a)</td><td>15 ms</td></tr><tr><td>f(f(a))</td><td>10 ms</td></tr><tr><td>f(f(f(a)))</td><td>5 ms</td></tr><tr><td>Time to fetch cache</td><td>1 ms</td></tr></tbody></table><p>Let&rsquo;s assume the following scenarios for caching the call stack.</p><table><thead><tr><th>Function Cached</th><th>Time</th><th>Time(ms)</th></tr></thead><tbody><tr><td>No Cache</td><td>f(a) + f(f(a)) + f(f(f(a)))</td><td>30 ms</td></tr><tr><td>f(f(a))</td><td>f(a) + f(f(a)) + Cache Time</td><td>26 ms</td></tr><tr><td>f(f(f(a)))</td><td>f(a) + Cache Time</td><td>16ms</td></tr><tr><td>f(a)</td><td>Cache Time</td><td>1ms</td></tr></tbody></table><p><em>Observation</em> : The higher we cache up the call stack the lesser functions are called and the time for resolving the function is reduced.</p><p><em><em>Hence</em></em>, as a thumb of rule caching high up the call stack is the best approch for caching in a system where function calls are chained and are a bottlneck in terms of time.</p><h1 id=when-to-expire-cache->When to expire cache ?</h1><p><strong>Scenario 1</strong>:
Assume you have a User A and you have a system where you have multiple inputs such as User A&rsquo;s Height, User A&rsquo;s Age, User A&rsquo;s Major and you have an heuristic algorithm where you determine if User A uses Arch Linux or not. And also look at a key-value cache as an example running sideways</p><p>Imagine two scenarios in this case.</p><p><em><strong>at T0</strong></em></p><table><thead><tr><th>Inputs</th><th>Time</th></tr></thead><tbody><tr><td>Age</td><td>22</td></tr><tr><td>Height</td><td>175 cms</td></tr><tr><td>Major</td><td>CS</td></tr></tbody></table><p><em><strong>Algorithm predicts that User A uses Arch Linux.</strong></em></p><p>Cache looks like this at the moment.</p><table><thead><tr><th>UserId</th><th>OS</th></tr></thead><tbody><tr><td>UserA</td><td>ArchLinux</td></tr></tbody></table><p><em><strong>at T1</strong></em></p><p>User A changes his major to design and now for some reason uses MacOS.</p><table><thead><tr><th>Inputs</th><th>Time</th></tr></thead><tbody><tr><td>Age</td><td>22</td></tr><tr><td>Height</td><td>175 cms</td></tr><tr><td>Major</td><td>Design</td></tr></tbody></table><p>If the cache looks like T0 then we will get a stale value for the OS of the user.</p><table><thead><tr><th>UserId</th><th>OS</th></tr></thead><tbody><tr><td>UserA</td><td>ArchLinux</td></tr></tbody></table><p><em><strong>Algorithm predicts that User A uses MacOS but according the the cache it&rsquo;s Arch.</strong></em></p><p>Now, we know that the user will be using MacOS but the cache will return <em><strong>Arch Linux.</strong></em> to the front end.</p><p><em><strong>So, we take a lesson that cache is always a side effect of the inputs that you put in a system, if you change the inputs you have to update the cache to the most recent result.</strong></em></p><p>You need to make sure to detect any inputs that the cache is a side effect of, if they change; update the cache. After the updation the cache looks like this</p><table><thead><tr><th>UserId</th><th>OS</th></tr></thead><tbody><tr><td>UserA</td><td>MacOS</td></tr></tbody></table><p><strong>Scenario 2</strong>:
Assume you make a really cool and hyped application and there are alot of new users logging in. You are caching some value for every user that get&rsquo;s calculated of user inputs. You have a large cache key. But for some reason the older users are not logging in anymore and the hype is dead. You are still storing cache of the old users and the current users that are still using the platform. Caching on RAM is expensive and useless cache means still bills from cloud providers.</p><p>Solution: You delete the cache for the users that are inactive and are not using the platform anymore; Thankfully for this kinf of invalidation you do not have to worry. Most caching technologies for example: redis support LRU iviction for deleting unused caches and keeping your cache hit ratio high.</p><h1 id=types-of-caching-technologies>Types of caching technologies</h1><ol><li><p>Browser Cache - The first and the most common type of caching layer built in the modern browsers is the browser cache.
Browser caches have ability to not send requests if the data is already cached. This can be seen in the request times even on sdplus.io site loading.</p></li><li><p>Caching Proxies - By an ISP. It is a read through cache that could cache traffic. THe larger the network the larger the potential cache hits.
Now they don&rsquo;t happen because of SSL the proxies do not have permissions to decrypt the payload and also bandwidth is much cheaper.</p></li><li><p>Reverse Proxies - This works like caching proxies only and is a read through cache, instead it is installed at the datacenter side (your server). They are an excellent way to scale and to speed up the service layer.</p></li><li><p>CDN (Content Delivery Network) - A group of distributed servers based of geolocation hosting content to users in different parts of globe.</p></li></ol><h1 id=caching-for-different-use-cases>Caching for different use cases</h1><ol><li>Read Intensive - When you have a application where the front end requires alot of data to be displayed from the multiple parameters.</li><li>Write Intensive - When you have an application where a large amounts of inputs are taken, computed and stored in the DB. (Something like a MBTI test ? Where you put multiple inputs and get a MBTI value.)</li></ol><h1 id=read-trough-and-write-through-cache>Read Trough and write through cache</h1><h2 id=read-through-cache>Read through cache</h2><p>The usecase if for a read intensive application
<img src=https://i.imgur.com/QVYG6WC.jpeg alt=image></p><h2 id=write-through-cache>Write through cache</h2><p>The usecase if for a write intensive application
<img src=https://i.imgur.com/3qtgsNd.jpeg alt=image></p><hr></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/caching>Caching</a></li><li><a href=/tags/redis>Redis</a></li><li><a href=/tags/memcached>Memcached</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://twitter.com/HiMadhavHere title=Twitter><i data-feather=twitter></i></a></div><div class=footer-info>2022 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script>feather.replace()</script></div></body></html>